Large language models (LLMs) have achieved impressive performance on multiple-choice question answering tasks in zero-shot settings. However, recent studies reveal that LLM predictions can be sensitive to the order in which answer options are presented, a phenomenon known as \textit{order bias}. In this work, we conduct a systematic analysis of order bias in LLMs using a bilingual (English-Chinese) multiple-choice-question (MCQ) dataset spanning 17 knowledge domains. After removing inconsistent question pairs, we evaluate 1,700 aligned examples under various prompting strategies using GPT-4o and mistral-small. Our findings confirm that order bias affects model outputs, even when semantic content is held constant. We further propose a simple calibration method, Cognitive Alignment, that mitigates this bias across option permutations. This study highlights a critical limitation in current LLM evaluation paradigms and provides practical guidance for designing more robust prompting methods.